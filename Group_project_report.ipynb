{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31c9838-e838-4f27-9e43-c0e096e59bad",
   "metadata": {},
   "source": [
    "# **Students' Knowledge Status**\n",
    "\n",
    "**Group: Liam Brennan, Eva He, Li-Kun Lin, Steve He** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8567e-3e9d-443b-a290-38fb1fcca3dc",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "The objective of any class should be to increase student’s understanding of the topic subject. Students with a high understanding of class material will enter the workforce with the tools they need to succeed in their relevant subjects. According to Papanikolaou, the professor of School of Pedagogical & Technological Education, “learners' knowledge level is used as valuable information to represent learners' current state” (Papanikolaou, et.al, 2002). Thus, we will build a KNN model to classify students' knowledge level based on five quantitative variables taken from Kahraman et al’s “User Knowledge Modeling dataset\".\n",
    "\n",
    "**Question**: Can we predict a student's knowledge level based on their study habits and exam performances using the Knn-classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ef0ef-02ef-4f8d-8a01-8221cd8ecb6b",
   "metadata": {},
   "source": [
    "Data Set information: \n",
    "The dataset was collected by Ka​​hraman et al. The weighting system and development of quantitative measurements for the variables was done using Kahramans rule based system which gives quantitative values(ratings) to students' performances in certain academic related parameters (Kharaman et al, 2013). The parameters:\n",
    "\n",
    "\n",
    "STG: Refers to Study time rating(0-1), the amount of time spent studying about Electrical DC Machines.\n",
    "\n",
    "SCG: Refers to Repetition rating(0-1),  the amount of problems, material the student worked on. For example, worksheets, tutorials.\n",
    "\n",
    "PEG: Refers to the exam performance rating of the subject(0-1), in this case, the exam performance on Electrical DC Machine course.\n",
    "\n",
    "STR: Refers to Study time rating of related subjects(0-1), the amount of time students spent studying related topics.\n",
    "\n",
    "LPR; Refers to exam performance rating in related subjects(0-1), exam performance on related material, or background information.\n",
    "\n",
    "*UNS(): Refers to student understanding levels; Based on the weighting system Kahramans uses in his rule based system paper, classified  as “Very-low” “Low”, “Middle”, or “High” Understanding of Electrical DC Machines(Kharaman, 2013 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b881f-fc4f-4e39-b139-40ab544bc888",
   "metadata": {},
   "source": [
    "## Methods & Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be55856-429e-4856-b135-dd04ff8a5978",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. loading in all the packages for data analysis\n",
    "This analysis utilizes the knn classification algorithm to predict the knowledge level (High, Middle, Low or Very Low) of students. First, all the packages that are necessary to perform this algorithm are loaded into R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48163a-2af9-45e9-9945-b235dfef88fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install.packages(\"themis\")\n",
    "install.packages(\"kknn\")\n",
    "install.packages(\"cowplot\")\n",
    "library(kknn)\n",
    "library(purrr)\n",
    "library(tidyverse)\n",
    "library(repr)\n",
    "library(tidymodels)\n",
    "library(themis)\n",
    "library(cowplot)\n",
    "options(repr.matrix.max.rows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0672b-228d-4bd9-8bc4-009d154dd4c8",
   "metadata": {},
   "source": [
    "### 2. Reading in the data from the web, and Preparation of Data analysis\n",
    "\n",
    "#### (A) loading the data\n",
    "After loading the data, we factorize the variable we want to predict, UNS, by using the function `as.factor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a99637-f678-45af-b949-c2269430277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1234) \n",
    "options(repr.plot.height = 5, repr.plot.width = 6)\n",
    "\n",
    "##Loading data\n",
    "url <- \"https://github.com/JackyLinllk/ubc_dsci100_assignment/raw/main/data/Data_User_Modeling_Dataset_full.csv\"\n",
    "knowledge_data<-read_csv(url)|>\n",
    "    select(STG:UNS)|>\n",
    "    mutate(UNS = as.factor(UNS))\n",
    "knowledge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afff09a-4242-47bd-b4ff-eef72ef8f863",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Table 1: Dataset Of Students' Knowledge Status\n",
    "    This table represents the students' knowledge status about the subject of Electrical DC Machines, encompassing key metrics such as Study Time Rating (STG), Repetition Rating (SCG), Study Time Rating of related subjects (STR), Exam Performance Rating in related subjects (LPR), Exam Performance Rating of the subject (LPR), and their overall Knowledge Level (UNS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d0424-fbf1-4b51-a37b-ef2f29cd0813",
   "metadata": {},
   "source": [
    "#### (B) NA-values\n",
    "We then check for NA-values, and realized that the last row restores NA-values across all the variables, so we just simply deleted the last row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd867a2-b42e-4045-9112-070b725a928e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Check NA rows\n",
    "na_row= which(!complete.cases(knowledge_data))\n",
    "print(na_row)\n",
    "\n",
    "##Delete the NA row of the data\n",
    "knowledge_data= knowledge_data[-nrow(knowledge_data),]\n",
    "knowledge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13853f31-e585-4fa9-a3a9-76dfab1dfda8",
   "metadata": {},
   "source": [
    "##### Table 2: Dataset Of Students' Knowledge Status with removed NA-Values\n",
    "    Data set after the NA-Values are removed, with the same variables(STG SCG STR LPR PEG UNS) as table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286f353-d420-471b-a7e5-34cd4da0273c",
   "metadata": {},
   "source": [
    "#### C) Preliminary Analysis and Final prep \n",
    "We check the data’s tidiness from \"Table 2\" by looking into 3 factors, which are: (1) each row is a single observation, (2) Each column is a single variable, (3) Each value is in single cell. \n",
    "\n",
    "\n",
    "After checking the data is tidy, we utilize function called\n",
    "group_by with summarize to find the summary statistics of the number of observations in\n",
    "each factor level with the corresponding proportion (Table 3).\n",
    "\n",
    "We realize that under the quantitative variable named UNS, the factor levels\n",
    "named “very_low” and “Very Low” should be in the same level. Thus, we apply the function\n",
    "mutate with function `fct_recode` to merge two factor levels into one.\n",
    "\n",
    "On top of that, we realized the\n",
    "classifier is class imbalance, which means the proportion for each stratum is not equally\n",
    "proportional. Thus, we apply the functions called `uc_recipe` and `step_upsample` to rebalance\n",
    "the rare class, namely “Very Low”, by oversampling. Then, we again utilize group_by with\n",
    "summarize to check each stratum is equally proportional to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1bfdc-a5bb-4d7e-9422-498ab55e312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the Summary table and checking for Proportion \n",
    "class_prop = knowledge_data|>\n",
    "  group_by(UNS)|>\n",
    "  summarize(count = n(),\n",
    "            percentage= count/nrow(knowledge_data))\n",
    "class_prop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea3358-9dcc-4b70-9289-68d7d052a083",
   "metadata": {},
   "source": [
    "##### Table 3: Summary Statistics of The number of Observations Knowledge Levels\n",
    "    \n",
    "    This table displays summary statistics for each knowledge level(UNS), including the count of observations(count) and the corresponding proportion(percentage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0d8d7-40f4-4262-bb13-101a6b304a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Typo in the dataset #very_low and Very Low should be same observation\n",
    "knowledge_data= knowledge_data|>\n",
    "  mutate(UNS = fct_recode(UNS, \"Very Low\" =  \"very_low\"))\n",
    "\n",
    "##Balancing the data\n",
    "ups_recipe= recipe(UNS~. ,data=knowledge_data)|>\n",
    "  step_upsample(UNS, over_ratio=1, skip=F)|>\n",
    "  prep()\n",
    "upsampled_knowledge=bake(ups_recipe, knowledge_data)\n",
    "\n",
    "##Checking the balance\n",
    "upsampled_knowledge|>\n",
    "  group_by(UNS)|>\n",
    "  summarize(n=n())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4a109-ad79-4098-9c9b-a5ad345324b9",
   "metadata": {},
   "source": [
    "##### Table 5: Summary Statistics of The number of Observations Knowledge Levels After Balancing the Data\n",
    "    This table presents summary statistics for each knowledge level (UNS) with the updated count of observations following the upsampling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2acd-63ea-4e94-81a5-6d98f134c5a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Splitting the data into 70% properation of training data and testing data\n",
    "\n",
    "#### A) Splitting the data \n",
    "We separated the data using the `initial_split` function to create 2 subsets, namely training set and testing set.\n",
    "Inside the initial_split function, we set the strata argument to the categorical variable UNS. The\n",
    "training and testing functions are used to create two different data frames with the\n",
    "corresponding weight of 70% and 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dc004-98a0-4db8-98f6-2c2673373f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the data into training set and testing set\n",
    "knowledge_data_split <- initial_split(upsampled_knowledge, prop = 0.70, strata = UNS)  \n",
    "knowledge_data_split_train <- training(knowledge_data_split)\n",
    "knowledge_data_split_test <- testing(knowledge_data_split)\n",
    "\n",
    "knowledge_data_split_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca7ba6-6a1d-426f-ac99-f454d3d44d44",
   "metadata": {},
   "source": [
    "##### Table 6: The Training set of Student's Knowledge \n",
    "    This table represents the training set we're utilizing for model training, encompassing variables such as STG SCG STR LPR PEG UNS(see intro for more info). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3868ebd-cde0-4ef9-857e-ebe061d20a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knowledge_data_split_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4d3ad-d35d-4cd3-b342-737973d3ca45",
   "metadata": {},
   "source": [
    "##### Table 7: The Testing set of Student's Knowledge \n",
    "    This table represents the testing set we're utilizing to evaluate our models with, encompassing variables such as STG SCG STR LPR PEG UNS(see intro for more info). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72ce64-3226-40b9-813a-d4c02dc8c642",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B) Preliminary Analysis on the Training set \n",
    "We created a Summary table of observations in each knowledge level using `group_by` UNS and `summarize`, to see if each of the Knowledge levels are balanced or not. We also created a plot that compares all the variables against PEG, setting it on the y-axis, and all the others on the X-axis. We did this because we wanted a base for comparison, so we used PEG as that base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a052e51-171a-4431-89ab-4b291f3d6ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the Summary table\n",
    "data_training_summary <- knowledge_data_split_train|>\n",
    "    group_by(UNS)|>\n",
    "    summarize(count = n())\n",
    "data_training_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415d558-dacc-4120-8596-b473bae58b50",
   "metadata": {},
   "source": [
    "##### Table 7: Summary Statistics of The number of Observations Knowledge Levels of Training data\n",
    "    This table represents the summary statistics of the number of observations(count) in each knowledge level(UNS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363fe99-c44f-424f-a4c5-940463f640cd",
   "metadata": {},
   "source": [
    "We see that the count of each Knowledge level is the same, and we won't have to worry about it being unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff218d0e-1fe7-4c50-96cc-a94374255eca",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we created the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b570782-72ec-410d-9056-b863e6c43ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.plot.height = 8, repr.plot.width = 15)\n",
    "# craeting plot for comparing PRG with LPR\n",
    "data_plot_LPR_PEG <- ggplot(knowledge_data_split_train, aes(x= LPR,y= PEG, color = UNS))+\n",
    "    geom_point() +\n",
    "    labs(x=\"Exam performance Rating \\n in Related Subject (0-1)\", y= \"Exam performance Rating \\n in Subject (0-1)\", color = \"Knowledge level\")+\n",
    "    ggtitle(\"Exam Performance vs Exam performance in Related Subject\")+\n",
    "    theme(text = element_text(size = 15))\n",
    "\n",
    "# craeting plot for comparing STG with LPR\n",
    "data_plot_STG_PEG <- ggplot(knowledge_data_split_train, aes(x= STG,y= PEG, color = UNS))+\n",
    "    geom_point() +\n",
    "    labs(x=\"Study time rating (0-1)\", y= \"Exam performance Rating \\n in Subject (0-1)\", color = \"Knowledge level\")+\n",
    "    ggtitle(\"Exam Performance vs Study time rating\")+\n",
    "    theme(text = element_text(size = 15))\n",
    "\n",
    "# craeting plot for comparing SCG with LPR\n",
    "data_plot_SCG_PEG <- ggplot(knowledge_data_split_train, aes(x= SCG,y= PEG, color = UNS))+\n",
    "    geom_point() +\n",
    "    labs(x=\"Repetition rating (0-1)\", y= \"Exam performance Rating \\n in Subject (0-1)\", color = \"Knowledge level\")+\n",
    "    ggtitle(\"Exam Performance vs Repetition rating \")+\n",
    "    theme(text = element_text(size = 15))\n",
    "\n",
    "# craeting plot for comparing STR with LPR\n",
    "data_plot_STR_PEG <- ggplot(knowledge_data_split_train, aes(x= STR,y= PEG, color = UNS))+\n",
    "    geom_point() +\n",
    "    labs(x=\"Study time rating \\n in Related Subject (0-1)\", y= \"Exam performance Rating \\n in Subject (0-1)\", color = \"Knowledge level\")+\n",
    "    ggtitle(\"Exam Performance vs Study time rating in Related Subject\")+\n",
    "    theme(text = element_text(size = 15))\n",
    "\n",
    "\n",
    "# putting all the plots into one\n",
    "compare_plot <- plot_grid(data_plot_LPR_PEG, data_plot_STG_PEG, data_plot_SCG_PEG, data_plot_STR_PEG, ncol = 2)\n",
    "compare_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d383f59-c61c-4ba5-a14e-75d05e16d59c",
   "metadata": {},
   "source": [
    "##### Plot 1: The Relationship Between the Variables  \n",
    "    This plot visualizes the relationships of PEG (Y-axis) with all other variables (excluding UNS), Colored by knowledge level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d7914-6565-45dc-80f8-9aa465aa1278",
   "metadata": {},
   "source": [
    "From plot 1, It is clear from these plots that PEG with any of the variables can distinguish between different knowledge levels. Although Some combinations are better than others, like the \"Exam performance vs exam performance in related subjects\", but generally speaking, all of them works. Therefore, any combination of PEG with any of the variables can be used as predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54033257-b10c-4291-8a73-bb797e232c74",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Parameter selection: Finding the best K value\n",
    "Our next step is to find the best K value and selecting the predictor variables  which maximizes the accuracy for our model.\n",
    "\n",
    "First, we apply nearest_neighbor, set_engine, and set_mode\n",
    "functions to create a model specification. Inside the `nearest_neighbor function`, the argument\n",
    "`weight_func` is set to rectangular, which means each k neighbor is equally important. For\n",
    "the neighbors argument, `tune()` is telling the framework to find the different parameter values\n",
    "for K (Timbers et al, 2023).\n",
    "\n",
    "For selecting the K variable, where K is the number of neighbors. We will be using cross-validation with a validation set of 5 in the training set to find the best possible k value. In other words, we will split our\n",
    "training data into 5 training sets. \n",
    "\n",
    "For cross-validation, we use `vfold_cv` function to set the validation set into 5 folds\n",
    "by using the training set. Finally, we create a tribble with neighbors and use the seq function to\n",
    "set the K-values to odd numbers (e.g., 1,3,5... 𝑛). The reason why we don’t want even\n",
    "numbers is because each neighbor is equally weighted; therefore, the even numbers will\n",
    "cause confusion (Timbers et al, 2023).\n",
    "\n",
    "For selecting the predictor variables for the recipe, we based it off of the research article by Kharaman 2013,(Developing Intuitive Knowledge Classifier). The article mentioned that in predicting the user knowledge levels, the most useful variable to consider are the Study time rating(STG), the Repetition rating( SCG), the exam performance rating(PEG) of the subject, the Study time rating(STR) of related subjects, and the exam performance rating in related subjects(LPR). Basically all the variables (Kahraman et al, 2013). And from Plot 1, we can also see that indeed distinctions between the different knowledge levels can be made with all the different variables. \n",
    "\n",
    "Since KNN classification uses Euclidean distance between points, it is very sensitive\n",
    "to the different types of scale. Thus, we planned to standardize the variables for all chosen\n",
    "variables to ensure the predictive algorithms are accurate and unbiased. We managed to\n",
    "standardize all the variables by using the recipe function with `step_center(all_predictors())` and\n",
    "`step_scale(all_predictors())`.\n",
    "\n",
    "Finally, we put everything into workflow to chain all the steps together to get the\n",
    "accuracy of different K-values. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d52f4e-485b-4118-9a6a-13cdc930ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Finding the k value for best accuracy\n",
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "##Choosing all the variables as predictors, and standardize it\n",
    "data_recipe <- recipe(UNS ~. , data = knowledge_data_split_train)|>\n",
    "  step_center(all_predictors())|>\n",
    "  step_scale(all_predictors())\n",
    "\n",
    "training_vfold <-  vfold_cv(knowledge_data_split_train, v=5, strata = UNS)\n",
    "\n",
    "k_value = 101\n",
    "K <- tibble(neighbors = seq(1,k_value,2))\n",
    "\n",
    "knn_result <- workflow() |>\n",
    "  add_recipe(data_recipe) |>\n",
    "  add_model(knn_tune)|>\n",
    "  tune_grid(resamples = training_vfold, grid = K) |>\n",
    "  collect_metrics()|>\n",
    "  filter(.metric == \"accuracy\")\n",
    "knn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d548a-8315-408d-8a1f-75c1439d695f",
   "metadata": {},
   "source": [
    "##### Table 8: Accuracy of different K values  \n",
    "    This table represents the Accuracy of different K values from 1 to 101, Advancing by 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1c516-514b-4406-99fc-4bab9ed02b0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Visualizing the optimal K-value\n",
    "\n",
    "We used the ggplot function to create a line graph which\n",
    "helps to visualize the accuracy trends under corresponding K-values. Surprisingly, when the\n",
    "K=1, we have the most accurate K-value for the model. Thus, we choose K equals to one as\n",
    "our optimal K-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78578a7d-c4bf-46c4-850d-f18041014dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scatter plot on the accuracy and number of neighbors\n",
    "options(repr.plot.height = 8, repr.plot.width = 8)\n",
    "cross_val_plot <- ggplot(knn_result, aes(x=neighbors, y= mean)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  labs(x = \"Neighbors\", y = \"Accuracy Estimate\") +\n",
    "  ggtitle(label= \"KNN Accuracy verses Number of Neighbors\")\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197522e1-5d9f-490f-b945-2c63a24a2b0b",
   "metadata": {},
   "source": [
    "##### Plot 2: KNN Accuracy versus Number of Neighbors\n",
    "    This plot represents the Virtualization of \"Table 7\". With the Accuracy Estimate on the y-axis, and Neighbors on the X-axis. \n",
    "    \n",
    "    Notably, the Highest accuracy came from a K neighbor of 1. So we will be using for K = 1 for the optimal K value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfae034-fcf7-4e67-a0ab-a0ecbf32860f",
   "metadata": {},
   "source": [
    "### 6. Creating the model with the optimal K-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fa137-6852-4ec6-99e7-54f0d24683e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "We Chose the K value based on Plot 2, where the highest accuracy is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9bc19-5097-49e5-9b06-5fedaebe4049",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Finding confusion matrix of model using testing set\n",
    "knn_best_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 1) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "\n",
    "knn_fit <- workflow() |>\n",
    "  add_recipe(data_recipe) |>\n",
    "  add_model(knn_best_tune)|>\n",
    "  fit(knowledge_data_split_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698f2fe-19dd-4207-986f-8376a14f61d0",
   "metadata": {},
   "source": [
    "### 7. Predicting the model on the testing data set, and evaluating the model\n",
    "\n",
    "#### A)  Predicting the on testing set\n",
    "We first used the model to predict the knowledge levels of the testing set (table 9).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016d3a9-8b2e-4f57-9c8c-b11f4a227daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Predicting the UNS of testing data set\n",
    "knowledge_predictions= knn_fit|>\n",
    "  predict(knowledge_data_split_test)|>\n",
    "  bind_cols(knowledge_data_split_test)\n",
    "knowledge_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591dceb-1e34-4cf6-aaff-1a1e5d14f07b",
   "metadata": {},
   "source": [
    "##### Table 9: Predicted knowledge levels\n",
    "    This table represents the predicted knowledge level generated by the model on the testing set (Table 7). It includes the same variables as in the testing set (STG, SCG, STR, LPR, PEG, UNS), along with an additional variable, .pred_class, representing the knowledge level assigned by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb9d66-b7a2-4eeb-9bc1-f10637eb0b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B)  Evaluating the model\n",
    "\n",
    "Next, we calculated the corresponding accuracy and set up the confusion matrix, setting the truth to the actual Knowledge level(UNS),\n",
    "and comparing it to what the model predicted(.pred_class).\n",
    " $$𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = \\frac{𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑐𝑜𝑟𝑟𝑒𝑐𝑡 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛} {𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f979d-398c-4b94-b63f-bf17eca1a5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knowledge_metrics= knowledge_predictions|>\n",
    "  metrics(truth= UNS, estimate = .pred_class)|>\n",
    "  filter(.metric== \"accuracy\")\n",
    "knowledge_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50466c8-1f1b-4ffd-945b-e5c0f16f2d78",
   "metadata": {},
   "source": [
    "##### Table 10: Accuracy of the model\n",
    "    This table represents the accuracy of our model with .estimate showing the estimated accuracy percentage, ie 𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑐𝑜𝑟𝑟𝑒𝑐𝑡 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛 / 𝑇𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠\n",
    "    \n",
    "    Looking at the value of the .estimate variable, it shows that our model has an estimated accuracy on the testing set of ~92.9%, which is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad585f-f765-43f1-9b4f-ad469a80c823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knowledge_conf_mat= knowledge_predictions|>\n",
    "  conf_mat(truth= UNS, estimate = .pred_class)\n",
    "knowledge_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb761f2-95ab-439c-858c-c66cd361ebae",
   "metadata": {},
   "source": [
    "##### Table 11: Confusion matrix \n",
    "    This table represents the true number of each Knowledge level, and the predicted number of each Knowledge level.\n",
    "    The leftmost column represents the predicted number of observations, and the topmost row represents the true number of observations.\n",
    "    \n",
    "    Looking at the table, The highest number of missed predictions of the Knowledge level came from the \"low\" class, \n",
    "    with a total of 8 missed predictions. All the other Knowledge levels had only one missed prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc997e0-206a-40ed-b2d5-462e463ad838",
   "metadata": {},
   "source": [
    "#### C) Visualization of result \n",
    "We decided to visualize the results using a bar graph. To be more specific, we are comparing the number of true observations for a knowledge level versus the amount of predicted observations for a knowledge level. We did this by first using the `group_by `and `summarize` functions to count the number of observations for each knowledge level, for each predicted (count) and true (count2) cases. Then, we combined the two tables into one and added a new column to identify if it's \"True\" or \"Pred\" (count3). The purpose of the new column is to differentiate (color) them in the bar plots. Then we created bar plots that compared the true class with the predicted class of that one knowledge level, e.g., pred_high vs High, pred_low vs low, and so on, and used `plot_grid` to put them beside each other.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36bc03-4432-4916-911b-d54f819c73d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repre.plot.width= 11, reper.plot.height = 9)\n",
    "\n",
    "#Number of Predicted cases \n",
    "count <- knowledge_predictions|>\n",
    "    group_by(.pred_class) |>\n",
    "    summarize(count = n())|>\n",
    "    rename(UNS = .pred_class)|>\n",
    "    mutate(UNS = fct_recode(UNS, pred_High = \"High\", pred_Low = \"Low\", pred_Middle = \"Middle\", pred_Very_low = \"Very Low\"))\n",
    "\n",
    "#Number of True cases \n",
    "count2 <- knowledge_predictions|>\n",
    "    group_by(UNS) |>\n",
    "    summarize(count = n())\n",
    "\n",
    "#Combining the Number of Predicted cases with Number of True cases \n",
    "count3 <- rbind(count, count2)|>\n",
    "    mutate(pred_or_true = UNS)|>\n",
    "     mutate(pred_or_true = fct_recode(UNS, Pred = \"pred_High\", Pred = \"pred_Low\", Pred = \"pred_Middle\", Pred = \"pred_Very_low\",\n",
    "                                     True = \"High\",\n",
    "                                     True = \"Low\",\n",
    "                                     True = \"Middle\",\n",
    "                                     True = \"Very Low\"))\n",
    "\n",
    "# craeting plot for comparing Predicted vs True for Knowledge level High\n",
    "Compare_High <- count3|>\n",
    "    filter(UNS == \"pred_High\" | UNS== \"High\") |>\n",
    "    ggplot(aes(x= UNS, y= count, fill=pred_or_true))+\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    xlab(\"Knowledge level\") +\n",
    "    ylab(\"Number of Observations\")+\n",
    "    labs(fill = \"Prediction vs True\")+\n",
    "    ggtitle(\"Predicted High vs True High\")\n",
    "\n",
    "# craeting plot for comparing Predicted vs True for Knowledge level Middle\n",
    "Compare_Middle <- count3|>\n",
    "    filter(UNS == \"pred_Middle\" | UNS== \"Middle\") |>\n",
    "    ggplot(aes(x= UNS, y= count, fill=pred_or_true))+\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    xlab(\"Knowledge level\") +\n",
    "    ylab(\"Number of Observations\")+\n",
    "    labs(fill = \"Prediction vs True\")+\n",
    "    ggtitle(\"Predicted Middle vs True Middle\")\n",
    "\n",
    "# craeting plot for comparing Predicted vs True for Knowledge level low\n",
    "Compare_Low <- count3|>\n",
    "    filter(UNS== \"pred_Low\" | UNS== \"Low\")|>\n",
    "    ggplot(aes(x= UNS, y= count, fill=pred_or_true))+\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    xlab(\"Knowledge level\") +\n",
    "    ylab(\"Number of Observations\")+\n",
    "    labs(fill = \"Prediction vs True\")+\n",
    "    ggtitle(\"Predicted Low vs True Low\")\n",
    "# craeting plot for comparing Predicted vs True for Knowledge level very low\n",
    "Compare_Very_Low <- count3|>\n",
    "    filter(UNS== \"pred_Very_low\" | UNS== \"Very Low\")|>\n",
    "    ggplot(aes(x= UNS, y= count, fill=pred_or_true))+\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    xlab(\"Knowledge level\") +\n",
    "    ylab(\"Number of Observations\")+\n",
    "    labs(fill = \"Prediction vs True\")+\n",
    "    ggtitle(\"Predicted Very Low vs True Very Low\")\n",
    "\n",
    "# putting the plots beside each other \n",
    "combine_plot <- plot_grid(Compare_High, Compare_Middle,Compare_Low, Compare_Very_Low, ncol = 2)\n",
    "combine_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b578-f508-44b1-a1ce-f4e45be8339a",
   "metadata": {},
   "source": [
    "##### Plot 3: Predicted vs True\n",
    "    This plot compares the amounts of true (blue) with the predicted number (red) of each class level. The four plots represent the four different knowledge levels, starting with 'High' at the top left and ending with 'Very Low' at the bottom right.\n",
    "\n",
    "Again, we see that the highest number of missed predictions of the knowledge level came from the 'Low' class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b9986-51d6-43c1-9407-9077a10c6c44",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    " \n",
    "After using the cross-validation for the validation sets of 5, we find the optimal hyperparameter k-value to be 1(Plot 2), and the testing set reveals that our KNN model has an accuracy rate of approximately 93%(Table 10). Therefore, our model prediction tends to correspond with the labeled category of student performance(Plot 3). Thus, in this data set, we can conclude that our model is a valid estimator for classifying the student performance.\n",
    "\n",
    "Our model’s predictions have not shown much discrepancy in the accuracy across the four categories(Table 11). However, it should be noted that our model has highest accuracy in predicting the category labeled as “High”, with only one missed prediction, and is less accurate in predicting the category labeled as “Low”, with 8 missed predictions in a total of 40 observations(Table 11). In other words, our testing data reveals a 20% miss label for the “Low” variable (Kharaman et al. 2013 2).  Although 8 is not large enough to be considered as a remarkable number, the cause for this difference is worth potential future investigation. One possible reason might be the upsampling, we increase the observation in “Low” from 50 to 129 which is approximately 258% of the original observation(table 5).  However, the calculation/method behind the upsampling is beyond this course. \n",
    "\n",
    "Since we are creating a multiple explanatory variables KNN model, another technique we would apply to improve the accuracy of this model is the forward selection. The forward selection helps to eliminate the variables that are less statistically significant, which might help to generalize the model and exclude from the multivariable penalty. Thus, we would possibly have a better estimation for the general cases. Using this method, we could also get insight of which one out of the five variables has more influence on an individual’s knowledge level in this dataset, which was a question originally proposed in our hypothesis.\n",
    "\n",
    "Overall, the result of the model seems valid with relatively high accuracy, however, we are aware of the model limitation on overfitting. Since our hyperparameter k-value is 1, we are suggesting the new observation will match with the nearest one. This might work extremely well in this specific dataset, but we don’t have a valid reason to say this model is generalized. Thus, we cannot guarantee the model will work well when we add new observations with more noise.\n",
    "\n",
    "This model provides valuable real-life applications as it could potentially be applied to the current education system. Compared to the traditional letter grade grading system, the system used in this dataset has more criteria that brings in a more diverse perspective to assessing student learning. Our algorithm could help bring this system into practical use by using the algorithm to assign the category instead of human assignment(educators, authorities etc.), which avoids subjective bias. This could also allow this grading system to be applied across different educational institutions as the category assignment is universal and objective, in which the influence that the discrepancy between each individual graders has on the result could be minimized. \n",
    " \n",
    "Looking at the potential real life application of this model, it leads the way to future questions such as: How can we adapt the model to predict the knowledge level for a data set that incorporates more variables? This question is worth investigation as the criterion for this grading system could be modified or extended to fit more educational needs of different institutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd4a48-3303-4d1f-9b20-612ffd3cd482",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "Kahraman, H., Colak, I., & Sagiroglu, S. (2013). User Knowledge Modeling. UCI Machine Learning Repository. https://doi.org/10.24432/C5231X\n",
    "\n",
    "Kahraman, H. T., Sagiroglu, S., & Colak, I. (2013). The development of an intuitive knowledge classifier and the modeling of domain-dependent data. Knowledge-Based Systems, 37, 283–295. https://doi.org/10.1016/j.knosys.2012.08.009\n",
    "\n",
    "Timbers, T., Campbell, T., & Lee, M. (2023). Data Science: A First Introduction. datasciencebook.ca. https://datasciencebook.ca/index.html\n",
    "\n",
    "Papanikolaou, K. A., Grigoriadou, M., Magoulas, G. D., & Kornilakis, H. (2002). Towards new forms of knowledge communication: the adaptive dimension of a web-based learning environment. Computers & Education, 39(4), 333–360. https://doi.org/10.1016/s0360-1315(02)00067-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9906af2-de51-4c03-b6e2-6e85f35f0da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
